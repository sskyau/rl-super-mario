{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4CE8QwTTIZP",
        "outputId": "8e14da90-8678-469a-83b6-473a69d94b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nes-py\n",
            "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 76 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.5)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=434263 sha256=4b717fa83413a3d2bda3b828bc73ebd98da08acc71cea667fa39c75e2f0bb2c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py\n",
            "Successfully installed nes-py-8.1.8\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nes-py>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros) (8.1.8)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.1.2->gym-super-mario-bros) (0.16.0)\n",
            "Installing collected packages: gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.2\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,490 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,696 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [953 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,268 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,135 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,947 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [996 kB]\n",
            "Get:23 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 13.8 MB in 4s (3,462 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.2-1).\n",
            "libxext6 is already the newest version (2:1.3.3-1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libgl1-mesa-glx\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 5,532 B of archives.\n",
            "After this operation, 79.9 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 20.0.8-0ubuntu1~18.04.1 [5,532 B]\n",
            "Fetched 5,532 B in 0s (67.1 kB/s)\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 155455 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "#!pip install nes-py==0.2.6\n",
        "!pip install nes-py\n",
        "!pip install gym-super-mario-bros\n",
        "!apt-get update\n",
        "!apt-get install ffmpeg libsm6 libxext6  -y\n",
        "!apt install -y libgl1-mesa-glx\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h431M2kVTK8J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import gym\n",
        "import numpy as np\n",
        "import collections \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9R9_kjU-TPUh"
      },
      "outputs": [],
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Downsamples image to 84x84\n",
        "    Greyscales image\n",
        "\n",
        "    Returns numpy array\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 240 * 256 * 3:\n",
        "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "def make_env(env):\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    env = ScaledFloatFrame(env)\n",
        "    return JoypadSpace(env, SIMPLE_MOVEMENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fN89wNQhTR3z"
      },
      "outputs": [],
      "source": [
        "from os import X_OK\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "class DDQNMario:\n",
        "\n",
        "    def __init__(self, state_space, action_space, mem_size, buffer_sample_size, gamma, lr, max_epsilon, min_epsilon, epsilon_decay, pretrained):\n",
        "      \n",
        "      self.pretrained = pretrained \n",
        "\n",
        "      # Learning hyperparameters\n",
        "      self.gamma = gamma\n",
        "      self.epsilon_decay = epsilon_decay\n",
        "      self.min_epsilon = min_epsilon\n",
        "      self.epsilon = max_epsilon\n",
        "      self.huber = keras.losses.Huber()\n",
        "\n",
        "      # Env hyperparameters\n",
        "      self.step = 0\n",
        "      self.state_space = state_space\n",
        "      self.num_actions = action_space\n",
        "\n",
        "      # Experience replay\n",
        "      self.mem_size = mem_size\n",
        "      self.sync_rate = 5000\n",
        "      self.buffer = 0\n",
        "      self.buffer_sample_size = buffer_sample_size\n",
        "\n",
        "      # CNN network hyperparameters\n",
        "      self.lr = lr\n",
        "      self.optimiser = Adam(learning_rate=self.lr, clipnorm=1.0)\n",
        "\n",
        "      # Init CNN networks\n",
        "      if self.pretrained:\n",
        "        # use pre-trained weights for the models\n",
        "        self.online = load_model('online')\n",
        "        self.target = load_model('target')\n",
        "\n",
        "      else:\n",
        "        self.online = self.create_model()\n",
        "        self.target = self.create_model()\n",
        "\n",
        "      # Init arrays for logging\n",
        "      if self.pretrained:\n",
        "\n",
        "        with open(\"ending_position.pkl\", \"rb\") as f:\n",
        "            self.last_position = pickle.load(f)\n",
        "        with open(\"buffer.pkl\", \"rb\") as f:\n",
        "            self.buffer = pickle.load(f)\n",
        "        with open(\"state_hist.pkl\", \"rb\") as f:\n",
        "            self.state_hist = pickle.load(f)\n",
        "        with open(\"action_hist.pkl\", \"rb\") as f:\n",
        "            self.action_hist = pickle.load(f)\n",
        "        with open(\"reward_hist.pkl\", \"rb\") as f:\n",
        "            self.reward_hist = pickle.load(f)\n",
        "        with open(\"nstate_hist.pkl\", \"rb\") as f:\n",
        "            self.next_state_hist = pickle.load(f)\n",
        "        with open(\"done_hist.pkl\", \"rb\") as f:\n",
        "            self.done_hist = pickle.load(f)\n",
        "\n",
        "      else:\n",
        "        \n",
        "        self.init_log()\n",
        "        self.last_position = 0\n",
        "\n",
        "    def create_model(self):\n",
        "      inputs = Input(shape=self.state_space, batch_size=self.buffer_sample_size)\n",
        "      x = Conv2D(32, 8, strides=4, activation='relu', name='conv1', padding='same')(inputs)\n",
        "      x = Conv2D(64, 4, strides=2, activation='relu', name='conv2', padding='same')(x)\n",
        "      x = Conv2D(64, 3, strides=1, activation='relu', name='conv3', padding='same')(x)\n",
        "      x = Flatten()(x)\n",
        "      x = Dense(512, activation='relu')(x)\n",
        "      action = Dense(self.num_actions, activation='softmax', name='action')(x) \n",
        "      model = tf.keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "      model.compile(optimizer=self.optimiser, loss=self.huber, metrics=[\"accuracy\"])\n",
        "      #model.summary()\n",
        "      return model\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      self.step += 1\n",
        "\n",
        "      # Epsilon-greedy policy #\n",
        "      # Exploration \n",
        "      if random.random() < self.epsilon:  \n",
        "        return random.randrange(self.num_actions)\n",
        "      \n",
        "      # Exploitation\n",
        "      else:\n",
        "        state_tensor = tf.convert_to_tensor(state)\n",
        "        # print('state tensor: ', state_tensor.shape)\n",
        "        #state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "        #print('state tensor 2: ', state_tensor)\n",
        "        action_probs = self.online(state_tensor, training=False)\n",
        "        # print(action_probs)\n",
        "        return tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "    def sync_models(self):\n",
        "      self.target.set_weights(self.online.get_weights())\n",
        "\n",
        "    def sample_exp(self):\n",
        "      sample_idx = random.choices(range(self.buffer), k=self.buffer_sample_size)\n",
        "\n",
        "      state = self.state_hist[sample_idx]\n",
        "      action = self.action_hist[sample_idx]\n",
        "      reward = self.reward_hist[sample_idx]\n",
        "      next_state = self.next_state_hist[sample_idx]\n",
        "      done = self.done_hist[sample_idx]\n",
        "      \n",
        "      return state, action, reward, next_state, done\n",
        "\n",
        "    def experience_replay(self):\n",
        "      \n",
        "      if self.step % self.sync_rate == 0:\n",
        "        self.sync_models()\n",
        "\n",
        "      if self.buffer < self.buffer_sample_size:\n",
        "        return \n",
        "\n",
        "      state_sample, action_sample, reward_sample, next_state_sample, done_sample = self.sample_exp()\n",
        "\n",
        "      future_rewards = self.target.predict(next_state_sample)\n",
        "      updated_q_values = reward_sample + self.gamma * tf.reduce_max(future_rewards, axis=0)\n",
        "      # updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "      # One-hot encoding\n",
        "      masked_actions = tf.one_hot(action_sample, self.num_actions)\n",
        "      \n",
        "      # print()\n",
        "      with tf.GradientTape() as tape:\n",
        "\n",
        "          # Forward pass\n",
        "          q_values = self.online(state_sample)\n",
        "          q_action = tf.reduce_sum(tf.multiply(q_values, masked_actions), axis=0)\n",
        "\n",
        "          # Use huber loss as the loss functiona of the CNN\n",
        "          loss = self.huber(updated_q_values, q_action)\n",
        "\n",
        "      # Backpropagation\n",
        "      grads = tape.gradient(loss, self.online.trainable_variables)\n",
        "      self.optimiser.apply_gradients(zip(grads, self.online.trainable_variables))\n",
        "      \n",
        "      # Update epsilon\n",
        "      self.epsilon = max(self.min_epsilon, self.epsilon*self.epsilon_decay)\n",
        "\n",
        "    def init_log(self):\n",
        "      self.state_hist = np.zeros((self.mem_size, *self.state_space))\n",
        "      self.action_hist = np.zeros((self.mem_size, 1))\n",
        "      self.reward_hist = np.zeros((self.mem_size, 1))\n",
        "      self.next_state_hist = np.zeros((self.mem_size, *self.state_space))\n",
        "      self.done_hist = np.zeros((self.mem_size, 1))\n",
        "\n",
        "    def log(self, state, action, reward, next_state, done):\n",
        "      \n",
        "      # Remove older transition memories\n",
        "      if len(self.action_hist) >= self.mem_size:\n",
        "          self.init_log()\n",
        "\n",
        "      self.state_hist[self.last_position] = state.float()\n",
        "      self.action_hist[self.last_position] = action\n",
        "      self.reward_hist[self.last_position] = reward.float()\n",
        "      self.next_state_hist[self.last_position] = next_state.float()\n",
        "      self.done_hist[self.last_position] = done.float()\n",
        "      self.last_position = (self.last_position + 1) % self.mem_size\n",
        "      self.buffer = min(self.buffer + 1, self.mem_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fvyBJB3LTVph"
      },
      "outputs": [],
      "source": [
        "def show_state(env, ep=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dh12HV81TW4M"
      },
      "outputs": [],
      "source": [
        "def run(training_mode, pretrained):\n",
        "   \n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0') # world 1, stage 1, standard ROM\n",
        "    env = make_env(env)  # Wraps the environment so that frames are grayscale \n",
        "    observation_space = env.observation_space.shape\n",
        "    action_space = env.action_space.n\n",
        "    agent = DDQNMario(state_space=observation_space,\n",
        "                      action_space=action_space,\n",
        "                      mem_size=30000,\n",
        "                      buffer_sample_size=32,\n",
        "                      gamma=0.90,\n",
        "                      lr=.00025,\n",
        "                      max_epsilon=1.0,\n",
        "                      min_epsilon=0.02,\n",
        "                      epsilon_decay=0.99,\n",
        "                      pretrained=pretrained)\n",
        "    \n",
        "    # Init\n",
        "    num_episodes = 100\n",
        "    burnin = int(num_episodes * 0.05)\n",
        "    env.reset()\n",
        "    total_rewards = []\n",
        "    \n",
        "    for ep_num in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        state = torch.Tensor([state])\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        while True:\n",
        "            if ep_num > burnin and not training_mode: #<- render while training\n",
        "                show_state(env, ep_num)\n",
        "\n",
        "            action = agent.choose_action(state)\n",
        "            # print('action: ', action)\n",
        "            steps += 1\n",
        "            \n",
        "            state_next, reward, terminal, info = env.step(int(action))\n",
        "            total_reward += reward\n",
        "            state_next = torch.Tensor([state_next])\n",
        "            reward = torch.tensor([reward]).unsqueeze(0)\n",
        "            \n",
        "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
        "            \n",
        "            if training_mode:\n",
        "                agent.log(state, action, reward, state_next, terminal)\n",
        "                agent.experience_replay()\n",
        "            \n",
        "            state = state_next\n",
        "            if terminal:\n",
        "                break\n",
        "        \n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "        print(\"Total reward after episode {} is {}\".format(ep_num + 1, total_rewards[-1]))\n",
        "        num_episodes += 1      \n",
        "    \n",
        "    if training_mode:\n",
        "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.last_position, f, protocol=4)\n",
        "        with open(\"buffer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.buffer, f, protocol=4)\n",
        "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
        "            pickle.dump(total_rewards, f, protocol=4)\n",
        "        with open(\"state_hist.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.state_hist, f, protocol=4)\n",
        "        with open(\"action_hist.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.action_hist, f, protocol=4)\n",
        "        with open(\"reward_hist.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.reward_hist, f, protocol=4)\n",
        "        with open(\"nstate_hist.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.next_state_hist, f, protocol=4)\n",
        "        with open(\"done_hist.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.done_hist, f, protocol=4)\n",
        "        \n",
        "        agent.online.save(\"online\")\n",
        "        agent.target.save(\"target\")\n",
        "\n",
        "    env.close()\n",
        "    \n",
        "    if num_episodes > burnin:\n",
        "        plt.title(\"Episodes trained vs. Average Rewards\")\n",
        "        plt.plot([0 for _ in range(burnin)] + \n",
        "            np.convolve(total_rewards, np.ones((burnin,))/burnin, mode=\"valid\").tolist())\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run(training_mode=True, pretrained=False)"
      ],
      "metadata": {
        "id": "rPNdhPcz1WEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cbd251-888e-4606-afc7-9b42d69b2e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CydQkW-_bOiu"
      },
      "outputs": [],
      "source": [
        "run(training_mode=False, pretrained=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "rl-super-mario-bros.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}