{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sskyau/rl-super-mario/blob/main/rl_super_mario_bros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4CE8QwTTIZP",
        "outputId": "508faaa2-e7b1-48ca-8671-e6692b4d54a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nes-py in /usr/local/lib/python3.7/dist-packages (8.1.8)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py) (0.16.0)\n",
            "Requirement already satisfied: gym-super-mario-bros in /usr/local/lib/python3.7/dist-packages (7.3.2)\n",
            "Requirement already satisfied: nes-py>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros) (8.1.8)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.1.2->gym-super-mario-bros) (0.16.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 74.6 kB in 2s (33.6 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.2-1).\n",
            "libxext6 is already the newest version (2:1.3.3-1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libgl1-mesa-glx is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "#!pip install nes-py==0.2.6\n",
        "!pip install nes-py\n",
        "!pip install gym-super-mario-bros\n",
        "!apt-get update\n",
        "!apt-get install ffmpeg libsm6 libxext6  -y\n",
        "!apt install -y libgl1-mesa-glx\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "h431M2kVTK8J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import gym\n",
        "import numpy as np\n",
        "import collections \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "from gym import wrappers\n",
        "from gym.wrappers.frame_stack import FrameStack\n",
        "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
        "from gym.wrappers.resize_observation import ResizeObservation\n",
        "from gym.wrappers import AtariPreprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "9R9_kjU-TPUh"
      },
      "outputs": [],
      "source": [
        "# class MaxAndSkipEnv(gym.Wrapper):\n",
        "#     def __init__(self, env=None, skip=4):\n",
        "#         \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "#         super(MaxAndSkipEnv, self).__init__(env)\n",
        "#         # most recent raw observations (for max pooling across time steps)\n",
        "#         self._obs_buffer = collections.deque(maxlen=2)\n",
        "#         self._skip = skip\n",
        "\n",
        "#     def step(self, action):\n",
        "#         total_reward = 0.0\n",
        "#         done = None\n",
        "#         for _ in range(self._skip):\n",
        "#             obs, reward, done, info = self.env.step(action)\n",
        "#             self._obs_buffer.append(obs)\n",
        "#             total_reward += reward\n",
        "#             if done:\n",
        "#                 break\n",
        "#         max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "#         return max_frame, total_reward, done, info\n",
        "\n",
        "#     def reset(self):\n",
        "#         \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
        "#         self._obs_buffer.clear()\n",
        "#         obs = self.env.reset()\n",
        "#         self._obs_buffer.append(obs)\n",
        "#         return obs\n",
        "\n",
        "\n",
        "# class ProcessFrame84(gym.ObservationWrapper):\n",
        "#     \"\"\"\n",
        "#     Downsamples image to 84x84\n",
        "#     Greyscales image\n",
        "\n",
        "#     Returns numpy array\n",
        "#     \"\"\"\n",
        "#     def __init__(self, env=None):\n",
        "#         super(ProcessFrame84, self).__init__(env)\n",
        "#         self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "#     def observation(self, obs):\n",
        "#         return ProcessFrame84.process(obs)\n",
        "\n",
        "#     def process(frame):\n",
        "#         if frame.size == 240 * 256 * 3:\n",
        "#             img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "#         else:\n",
        "#             assert False, \"Unknown resolution.\"\n",
        "#         img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "#         resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "#         x_t = resized_screen[18:102, :]\n",
        "#         x_t = np.reshape(x_t, [84, 84, 1])\n",
        "#         return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "# class ImageToPyTorch(gym.ObservationWrapper):\n",
        "#     def __init__(self, env):\n",
        "#         super(ImageToPyTorch, self).__init__(env)\n",
        "#         old_shape = self.observation_space.shape\n",
        "#         self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "#                                                 dtype=np.float32)\n",
        "\n",
        "#     def observation(self, observation):\n",
        "#         return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "# class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "#     \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
        "#     def observation(self, obs):\n",
        "#         return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# class BufferWrapper(gym.ObservationWrapper):\n",
        "#     def __init__(self, env, n_steps, dtype=np.float32):\n",
        "#         super(BufferWrapper, self).__init__(env)\n",
        "#         self.dtype = dtype\n",
        "#         old_space = env.observation_space\n",
        "#         self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "#                                                 old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "#         return self.observation(self.env.reset())\n",
        "\n",
        "#     def observation(self, observation):\n",
        "#         self.buffer[:-1] = self.buffer[1:]\n",
        "#         self.buffer[-1] = observation\n",
        "#         return self.buffer\n",
        "\n",
        "\n",
        "# def make_env(env, frame_stack):\n",
        "#     env = MaxAndSkipEnv(env)\n",
        "#     env = ProcessFrame84(env)\n",
        "#     env = ImageToPyTorch(env)\n",
        "#     env = BufferWrapper(env, 4)\n",
        "#     env = ScaledFloatFrame(env)\n",
        "#     return JoypadSpace(env, SIMPLE_MOVEMENT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom class to make end of life = end of episode\n",
        "class SkipFrame(gym.Wrapper):\n",
        "\n",
        "  def __init__(self, env, n):\n",
        "    super(SkipFrame, self).__init__(env)\n",
        "    self._n = n\n",
        "  \n",
        "  def step(self, action):\n",
        "    sum_rewards = 0\n",
        "    \n",
        "    # Re-run every step in the n-frames and accumulate the info to return\n",
        "    done = False\n",
        "    for i in range(self._n):\n",
        "      #print('before step: ', done)\n",
        "      state, reward, done, info = self.env.step(action)\n",
        "      #print('after step: ', done)\n",
        "      sum_rewards += reward\n",
        "      if done:\n",
        "        break\n",
        "  \n",
        "    return state, sum_rewards, done, info\n",
        "\n",
        "class CustomReward(gym.Wrapper):\n",
        "  def __init__(self, env):\n",
        "    super(CustomReward, self).__init__(env)\n",
        "    self.score = 0\n",
        "\n",
        "  def step(self, action):\n",
        "    state, reward, done, info = self.env.step(action)\n",
        "    # original reward function: (new_x_pos - old_x_pos) + (new_time - old_time) + dealth_pealty(-15 if dead; 0 otherwise)\n",
        "    # clipped at (-15,15)\n",
        "\n",
        "    # Amplify rewards (10x)\n",
        "    if reward > 0:\n",
        "      reward *= 10\n",
        "    else:\n",
        "      reward -= 1\n",
        "      reward *= 10\n",
        "    \n",
        "    # info: {'coins': (int), \n",
        "    #        'flag_get': (bool), \n",
        "    #        'life': (int), \n",
        "    #        'stage': (int), \n",
        "    #        'status': (str) {'small', 'tall', 'firecall'}, \n",
        "    #        'time': (int) time left, \n",
        "    #        'world': (int) {1,...,8}, \n",
        "    #        'x_pos': (int), \n",
        "    #        'y_pos': (int)}\n",
        "\n",
        "           \n",
        "    if info['flag_get']:\n",
        "      reward += 500\n",
        "    \n",
        "    if info['status'] == 'tall':\n",
        "      reward += 100\n",
        "    \n",
        "    return state, reward, done, info\n",
        "\n",
        "class ScaleReshapeEnv(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ScaleReshapeEnv, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, \n",
        "                                                high=255.0, \n",
        "                                                shape=(old_shape[1], old_shape[2], old_shape[0]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = np.array(observation).astype(np.float32) / 255.0\n",
        "        # print('before conversion ', observation.shape)\n",
        "        shape = self.observation_space.shape\n",
        "        observation = np.reshape(observation, (shape))\n",
        "        return observation\n",
        "\n",
        "\n",
        "def make_env(env, frame_size, frame_skip, frame_stack):\n",
        "  env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "  # print('original: ', env.observation_space.shape)\n",
        "  env = CustomReward(env)\n",
        "  env = SkipFrame(env, frame_skip)\n",
        "  # print('after skip frame: ', env.observation_space.shape)\n",
        "  env = GrayScaleObservation(env)\n",
        "  # print('after grayscale: ', env.observation_space.shape)\n",
        "  env = ResizeObservation(env, frame_size)\n",
        "  # print('after resize: ', env.observation_space.shape)\n",
        "  env = FrameStack(env, frame_stack)\n",
        "  # print('after frame stack: ', env.observation_space.shape)\n",
        "  env = ScaleReshapeEnv(env)\n",
        "  # print('after scale reshape: ', env.observation_space.shape)\n",
        "  return env"
      ],
      "metadata": {
        "id": "t7G2LZdyXqGG"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UniformExperienceReplay(object):\n",
        "  def __init__(self, replay_size, mem_size):\n",
        "    self.sample_size = replay_size\n",
        "    self.mem_size = mem_size\n",
        "    self.buffer = [None] * self.mem_size\n",
        "    self.index = 0\n",
        "  \n",
        "  def push(self, s, a, r, d, s2):\n",
        "    self.buffer[self.index] = s, a, r, d, s2\n",
        "    self.index += 1\n",
        "\n",
        "    # reset if memory is full\n",
        "    if self.index == self.mem_size:\n",
        "      self.index = 0\n",
        "      self.buffer = [None] * self.mem_size\n",
        "  \n",
        "  def sample_exp(self):\n",
        "    s = [None] * self.sample_size\n",
        "    a = [None] * self.sample_size\n",
        "    r = [None] * self.sample_size\n",
        "    d = [None] * self.sample_size\n",
        "    s2 = [None] * self.sample_size\n",
        "\n",
        "    for batch, sample in enumerate(np.random.randint(0, self.index-1, self.sample_size)):\n",
        "      s_sample, a_sample, r_sample, d_sample, s2_sample = self.buffer[sample]\n",
        "      s[batch] = np.array(s_sample)\n",
        "      a[batch] = a_sample\n",
        "      r[batch] = r_sample\n",
        "      d[batch] = d_sample\n",
        "      s2[batch] = np.array(s2_sample)\n",
        "    \n",
        "    return np.array(s), np.array(a, dtype=int), np.array(r, dtype=float), np.array(d, dtype=bool), np.array(s2)\n"
      ],
      "metadata": {
        "id": "Lwzpafmhrp9S"
      },
      "execution_count": 332,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "fN89wNQhTR3z"
      },
      "outputs": [],
      "source": [
        "from os import X_OK\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Lambda, Multiply\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.backend import backend as K\n",
        "\n",
        "class DDQNMario:\n",
        "\n",
        "    def __init__(self, state_space, action_space, mem_size, frame_stack, replay_size, gamma, lr, max_epsilon, min_epsilon, epsilon_decay, sync_rate, pretrained, burnin):\n",
        "      \n",
        "      self.pretrained = pretrained \n",
        "\n",
        "      # Learning variables\n",
        "      self.gamma = gamma\n",
        "      self.epsilon_decay = epsilon_decay\n",
        "      self.min_epsilon = min_epsilon\n",
        "      self.epsilon = max_epsilon\n",
        "      self.huber = keras.losses.Huber()\n",
        "      self.burnin = burnin\n",
        "\n",
        "      # Env variables\n",
        "      self.step = 0\n",
        "      self.state_space = state_space\n",
        "      self.num_actions = action_space\n",
        "      self.frame_stack = frame_stack\n",
        "\n",
        "      # Experience replay variables\n",
        "      self.mem_size = mem_size\n",
        "      self.sync_rate = sync_rate\n",
        "      self.buffer = 0\n",
        "      self.replay_size = replay_size\n",
        "      self.replayer = UniformExperienceReplay(replay_size, self.mem_size)\n",
        "      self.action_mask = np.ones(self.num_actions, dtype=int)\n",
        "\n",
        "\n",
        "      # CNN network variables\n",
        "      self.lr = lr\n",
        "      self.optimiser = Adam(learning_rate=self.lr, clipnorm=1.0)\n",
        "      self.online_hist = []\n",
        "\n",
        "      # Init CNN networks\n",
        "      if self.pretrained:\n",
        "        # (Testing) use pre-trained weights for the models\n",
        "        self.online = load_model('online')\n",
        "        self.target = load_model('target')\n",
        "\n",
        "      else:\n",
        "        # (Training) inti models\n",
        "        self.online = self.create_model()\n",
        "        self.target = self.create_model()\n",
        "\n",
        "      # Logging\n",
        "      self.last_position = 0\n",
        "      self.epi_exploitation = 0\n",
        "\n",
        "    def create_model(self):\n",
        "      inputs = Input(shape=(self.state_space))\n",
        "      x = Conv2D(32, 3, strides=4, activation='relu', name='conv1')(inputs)\n",
        "      x = Conv2D(64, 3, strides=2,  activation='relu', name='conv2')(x)\n",
        "      x = Conv2D(64, 3, strides=1, activation='relu', name='conv3')(x)\n",
        "      x = Flatten()(x)\n",
        "      x = Dense(1024, activation='relu')(x)\n",
        "      action = Dense(self.num_actions, activation='tanh', name='action')(x) \n",
        "\n",
        "      model = tf.keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "      model.compile(optimizer=self.optimiser, loss=self.huber)\n",
        "      #model.summary()\n",
        "      return model\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      self.step += 1\n",
        "\n",
        "      # Epsilon-greedy policy #\n",
        "      # Exploration \n",
        "      if random.random() < self.epsilon:  \n",
        "        return random.randrange(self.num_actions)\n",
        "      \n",
        "      # Exploitation\n",
        "      else:\n",
        "        state = state[np.newaxis, :, :, :] # (84,84,4) --> (1,84,84,4)\n",
        "        actions = self.online.predict(state)\n",
        "        # print(np.argmax(actions))\n",
        "        self.epi_exploitation += 1\n",
        "\n",
        "        return np.argmax(actions[0])\n",
        "\n",
        "    def sync_models(self):\n",
        "      self.target.set_weights(self.online.get_weights())\n",
        "\n",
        "    def experience_replay(self, ep_num):\n",
        "      # reference: https://colab.research.google.com/github/ehennis/ReinforcementLearning/blob/master/06-DDQN.ipynb#scrollTo=4oVZuEP5vVTP\n",
        "      if self.step % self.sync_rate == 0:\n",
        "        self.sync_models()\n",
        "\n",
        "      if self.replayer.index < self.replayer.sample_size:\n",
        "        return \n",
        "\n",
        "      s = []\n",
        "      y = [] \n",
        "\n",
        "      # sample minibatch of state, action, reward, done and state2 \n",
        "      state_sample, action_sample, reward_sample, done_sample, state2_sample = self.replayer.sample_exp()\n",
        "\n",
        "      # predict in batch\n",
        "      state_pred_online = self.online.predict(state_sample) \n",
        "      state2_pred_online = self.online.predict(state2_sample)\n",
        "      state2_pred_target = self.target.predict(state2_sample)\n",
        "\n",
        "      for i in range(len(action_sample)):\n",
        "        s.append(state_sample[i])\n",
        "        \n",
        "        state2_action_pred_online = state2_pred_online[i]\n",
        "        state2_action_pred_target = state2_pred_target[i]\n",
        "      \n",
        "        if done_sample[i]:\n",
        "          target = reward_sample[i] # y_j = r_j + 0 if terminal\n",
        "        else:\n",
        "          target = reward_sample[i] + self.gamma * state2_action_pred_target[np.argmax(state2_action_pred_online)] \n",
        "        \n",
        "        # y_j\n",
        "        target_f = state_pred_online[i] # prediction result (array of size 7)\n",
        "        target_f[action_sample[i]] = target # \n",
        "        y.append(target_f)\n",
        "\n",
        "      s = np.array(s)\n",
        "      y = np.array(y)\n",
        "      \n",
        "      # perform single gradient update based on y_j\n",
        "      self.online_hist = self.online.train_on_batch(s, y)\n",
        "\n",
        "      # Update epsilon (only after burnin, let agent explore more before exploitation)\n",
        "      if ep_num >= self.burnin:\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon*self.epsilon_decay)\n",
        "\n",
        "    def log(self, state, action, reward, done, state2):\n",
        "      # print('shape of state logged: ', state.shape)\n",
        "      self.replayer.push(state, action, reward, done, state2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "fvyBJB3LTVph"
      },
      "outputs": [],
      "source": [
        "def show_state(env, ep=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
        "    plt.axis('off')\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "id": "dh12HV81TW4M"
      },
      "outputs": [],
      "source": [
        "frame_stack = 4\n",
        "def run(training_mode, pretrained, stage):\n",
        "   \n",
        "\n",
        "\n",
        "    env = gym_super_mario_bros.make(stage) # world 1, stage 1, standard ROM\n",
        "    env = make_env(env, \n",
        "                   frame_size=84,\n",
        "                   frame_skip=4,\n",
        "                   frame_stack=frame_stack)  # Wraps the environment so that frames are grayscale \n",
        "    state_space = env.observation_space.shape\n",
        "    # print('observation space: ', env.observation_space.shape)\n",
        "    action_space = env.action_space.n\n",
        "\n",
        "    # Init\n",
        "    num_episodes = 200\n",
        "    burnin = int(num_episodes * 0.05)\n",
        "    env.reset()\n",
        "    total_rewards = []\n",
        "    exploitation_rate = []\n",
        "\n",
        "    agent = DDQNMario(state_space=(84,84,4),\n",
        "                      action_space=action_space,\n",
        "                      frame_stack=frame_stack,\n",
        "                      mem_size=10000,\n",
        "                      replay_size=32,\n",
        "                      gamma=0.90,\n",
        "                      lr=.00025,\n",
        "                      max_epsilon=1.0,\n",
        "                      min_epsilon=0.1,\n",
        "                      epsilon_decay=0.9999,\n",
        "                      sync_rate = 10000, \n",
        "                      burnin = burnin,\n",
        "                      pretrained=pretrained)\n",
        "    \n",
        "    \n",
        "    \n",
        "    for ep_num in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        # print(state)\n",
        "        state = state\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        agent.epi_exploitation = 0\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if not training_mode: \n",
        "                show_state(env, ep_num)\n",
        "            # print('choose action')\n",
        "            action = agent.choose_action(state)\n",
        "            # print('state shape fed into choose action',state.shape)\n",
        "            steps += 1\n",
        "            \n",
        "            state2, reward, done, info = env.step(int(action))\n",
        "            # print(state2)\n",
        "            # print('env shape', env.observation_space.shape)\n",
        "            # print('shape of state returned in step:', state2.shape)\n",
        "            total_reward += reward\n",
        "            state2 = state2\n",
        "            \n",
        "            if training_mode:\n",
        "                agent.log(state, action, reward, done, state2)\n",
        "                agent.experience_replay(ep_num\n",
        "                                        )\n",
        "            \n",
        "            state = state2\n",
        "            if done:\n",
        "                exploitation_rate.append(agent.epi_exploitation / steps)\n",
        "                break\n",
        "        \n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "        print(\"Total reward after episode {} is {}. Exploitation rate is {}%.\".format(ep_num + 1, total_rewards[-1], exploitation_rate[ep_num]*100))\n",
        "        num_episodes += 1      \n",
        "    \n",
        "    if training_mode:\n",
        "        # save training record \n",
        "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.last_position, f, protocol=4)\n",
        "        with open(\"buffer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.replayer.buffer, f, protocol=4)\n",
        "        \n",
        "        agent.online.save(\"online\")\n",
        "        agent.target.save(\"target\")\n",
        "\n",
        "    env.close()\n",
        "    \n",
        "    if num_episodes > burnin:\n",
        "        plt.title(\"Episodes trained vs. Average Rewards\")\n",
        "        plt.plot([0 for _ in range(burnin)] + \n",
        "            np.convolve(total_rewards, np.ones((burnin,))/burnin, mode=\"valid\").tolist())\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run(training_mode=True, pretrained=False, stage='SuperMarioBros-1-4-v0')"
      ],
      "metadata": {
        "id": "rPNdhPcz1WEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf09ffd7-4c16-45ff-b385-f6f5413263f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/200 [00:01<04:43,  1.43s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 1 is 1370. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 2/200 [00:20<38:08, 11.56s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 2 is 3020. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 3/200 [00:42<54:35, 16.63s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 3 is 2040. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 4/200 [00:57<51:37, 15.80s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 4 is 3360. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▎         | 5/200 [01:19<59:17, 18.24s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 5 is 2570. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 6/200 [01:35<55:56, 17.30s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 6 is 3290. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 7/200 [01:45<48:11, 14.98s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 7 is 1220. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 8/200 [01:54<41:58, 13.12s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 8 is 1160. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 9/200 [02:08<42:37, 13.39s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 9 is 3330. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 10/200 [02:30<51:00, 16.11s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 10 is 2080. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 11/200 [02:56<59:51, 19.00s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 11 is 2200. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 12/200 [03:07<51:42, 16.50s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 12 is 970. Exploitation rate is 0.0%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▋         | 13/200 [03:27<55:03, 17.66s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 13 is 2610. Exploitation rate is 1.1764705882352942%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 14/200 [03:43<52:48, 17.03s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 14 is 3830. Exploitation rate is 1.4492753623188406%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 15/200 [03:58<50:54, 16.51s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 15 is 3780. Exploitation rate is 5.970149253731343%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 16/200 [04:17<52:50, 17.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 16 is 2570. Exploitation rate is 3.5294117647058822%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 17/200 [04:33<51:29, 16.88s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 17 is 3070. Exploitation rate is 2.857142857142857%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 18/200 [04:48<49:16, 16.24s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 18 is 3610. Exploitation rate is 3.125%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|▉         | 19/200 [05:06<50:59, 16.90s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 19 is 2700. Exploitation rate is 3.75%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 20/200 [05:14<42:55, 14.31s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 20 is 1110. Exploitation rate is 2.7027027027027026%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 21/200 [05:28<42:04, 14.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 21 is 800. Exploitation rate is 6.779661016949152%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 22/200 [05:53<51:23, 17.32s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 22 is 2360. Exploitation rate is 12.380952380952381%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 23/200 [06:11<52:15, 17.72s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 23 is 3190. Exploitation rate is 11.39240506329114%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 24/200 [06:24<47:32, 16.21s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 24 is 790. Exploitation rate is 5.454545454545454%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▎        | 25/200 [06:39<46:24, 15.91s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 25 is 320. Exploitation rate is 7.575757575757576%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 26/200 [06:48<39:51, 13.74s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 26 is 1390. Exploitation rate is 2.7027027027027026%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▎        | 27/200 [07:07<44:16, 15.36s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 27 is 3340. Exploitation rate is 10.975609756097562%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 28/200 [07:16<38:27, 13.42s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 28 is 1100. Exploitation rate is 13.157894736842104%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 29/200 [07:33<41:40, 14.62s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 29 is 3650. Exploitation rate is 11.11111111111111%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 30/200 [07:55<47:01, 16.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 30 is 6240. Exploitation rate is 11.235955056179774%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 31/200 [08:15<50:02, 17.77s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 31 is 2480. Exploitation rate is 15.11627906976744%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 32/200 [08:25<42:57, 15.34s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 32 is 1200. Exploitation rate is 9.75609756097561%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▋        | 33/200 [08:56<56:04, 20.15s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 33 is 1790. Exploitation rate is 18.75%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 34/200 [09:12<51:59, 18.79s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 34 is 360. Exploitation rate is 15.151515151515152%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 35/200 [09:28<49:51, 18.13s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 35 is 220. Exploitation rate is 31.343283582089555%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 36/200 [09:39<43:36, 15.95s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 36 is 1120. Exploitation rate is 17.391304347826086%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 37/200 [09:55<43:03, 15.85s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 37 is 470. Exploitation rate is 21.53846153846154%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▉        | 38/200 [10:05<37:51, 14.02s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 38 is 1220. Exploitation rate is 17.073170731707318%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|█▉        | 39/200 [10:15<34:58, 13.04s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 39 is 1080. Exploitation rate is 10.869565217391305%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 40/200 [10:32<38:00, 14.26s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 40 is 180. Exploitation rate is 16.216216216216218%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 41/200 [10:47<37:36, 14.19s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 41 is 660. Exploitation rate is 16.129032258064516%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██        | 42/200 [10:55<32:54, 12.49s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 42 is 1330. Exploitation rate is 16.666666666666664%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 43/200 [11:06<31:40, 12.10s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 43 is 840. Exploitation rate is 16.3265306122449%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 44/200 [11:20<32:35, 12.54s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 44 is 800. Exploitation rate is 24.561403508771928%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▎       | 45/200 [11:31<31:08, 12.05s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 45 is 1010. Exploitation rate is 21.73913043478261%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 46/200 [11:41<29:17, 11.41s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 46 is 1240. Exploitation rate is 23.809523809523807%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▎       | 47/200 [12:01<36:00, 14.12s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 47 is 3100. Exploitation rate is 27.586206896551722%.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 48/200 [12:18<37:34, 14.83s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 48 is 3430. Exploitation rate is 16.901408450704224%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 49/200 [12:30<35:32, 14.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward after episode 49 is 820. Exploitation rate is 20.37037037037037%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 50/200 [12:53<41:39, 16.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward after episode 50 is 2940. Exploitation rate is 23.958333333333336%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 51/200 [13:09<41:20, 16.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward after episode 51 is 3260. Exploitation rate is 20.833333333333336%.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 52/200 [13:23<39:14, 15.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward after episode 52 is 570. Exploitation rate is 28.333333333333332%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "CydQkW-_bOiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "outputId": "6b6f2607-545e-4eaf-d661-b6c2e180df73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 1/200 [00:08<27:35,  8.32s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-313-dcc9a4cff311>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SuperMarioBros-1-4-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-308-433e0051e17e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(training_mode, pretrained, stage)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtraining_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mshow_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# print('choose action')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-286-1262ca649384>\u001b[0m in \u001b[0;36mshow_state\u001b[0;34m(env, ep, info)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                            else suppress())\n\u001b[1;32m   2099\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m                     bbox_inches = self.figure.get_tightbbox(renderer,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1736\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2630\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2632\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 626\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             magnification, unsampled=unsampled)\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    522\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[1;32m    523\u001b[0m                 output = _resample(  # resample rgb channels\n\u001b[0;32m--> 524\u001b[0;31m                     self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_alpha\u001b[0m  \u001b[0;31m# recombine rgb and alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filternorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                     image_obj.get_filterrad())\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "run(training_mode=False, pretrained=True, stage='SuperMarioBros-1-4-v0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZHimu5_XQj5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "rl-super-mario-bros.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}