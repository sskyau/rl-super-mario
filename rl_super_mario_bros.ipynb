{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sskyau/rl-super-mario/blob/main/rl_super_mario_bros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4CE8QwTTIZP",
        "outputId": "b96aa07b-afa2-402f-c28d-3e2f0af78d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nes-py\n",
            "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 76 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py) (4.64.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=439813 sha256=5cec1ae3606cd1f9fd38847ea266a175253303c1825cd2d669be569af9f0843f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py\n",
            "Successfully installed nes-py-8.1.8\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.3.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nes-py>=8.1.2 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros) (8.1.8)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.21.6)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.2->gym-super-mario-bros) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.2->gym-super-mario-bros) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.1.2->gym-super-mario-bros) (0.16.0)\n",
            "Installing collected packages: gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.2\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [907 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,496 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,728 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,165 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [941 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,950 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [999 kB]\n",
            "Fetched 14.7 MB in 3s (4,306 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.2-1).\n",
            "libxext6 is already the newest version (2:1.3.3-1).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libgl1-mesa-glx\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 5,532 B of archives.\n",
            "After this operation, 79.9 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 20.0.8-0ubuntu1~18.04.1 [5,532 B]\n",
            "Fetched 5,532 B in 0s (66.8 kB/s)\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 155501 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "#!pip install nes-py==0.2.6\n",
        "!pip install nes-py\n",
        "!pip install gym-super-mario-bros\n",
        "!apt-get update\n",
        "!apt-get install ffmpeg libsm6 libxext6  -y\n",
        "!apt install -y libgl1-mesa-glx\n",
        "!pip install opencv-python\n",
        "# !pip install cupy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h431M2kVTK8J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import gym_super_mario_bros\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "import gym\n",
        "import numpy as np\n",
        "# import cupy as np\n",
        "import collections \n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "%matplotlib inline\n",
        "from gym import wrappers\n",
        "from gym.wrappers.frame_stack import FrameStack\n",
        "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
        "from gym.wrappers.resize_observation import ResizeObservation\n",
        "from gym.wrappers import AtariPreprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R9_kjU-TPUh"
      },
      "outputs": [],
      "source": [
        "# class MaxAndSkipEnv(gym.Wrapper):\n",
        "#     def __init__(self, env=None, skip=4):\n",
        "#         \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "#         super(MaxAndSkipEnv, self).__init__(env)\n",
        "#         # most recent raw observations (for max pooling across time steps)\n",
        "#         self._obs_buffer = collections.deque(maxlen=2)\n",
        "#         self._skip = skip\n",
        "\n",
        "#     def step(self, action):\n",
        "#         total_reward = 0.0\n",
        "#         done = None\n",
        "#         for _ in range(self._skip):\n",
        "#             obs, reward, done, info = self.env.step(action)\n",
        "#             self._obs_buffer.append(obs)\n",
        "#             total_reward += reward\n",
        "#             if done:\n",
        "#                 break\n",
        "#         max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "#         return max_frame, total_reward, done, info\n",
        "\n",
        "#     def reset(self):\n",
        "#         \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
        "#         self._obs_buffer.clear()\n",
        "#         obs = self.env.reset()\n",
        "#         self._obs_buffer.append(obs)\n",
        "#         return obs\n",
        "\n",
        "\n",
        "# class ProcessFrame84(gym.ObservationWrapper):\n",
        "#     \"\"\"\n",
        "#     Downsamples image to 84x84\n",
        "#     Greyscales image\n",
        "\n",
        "#     Returns numpy array\n",
        "#     \"\"\"\n",
        "#     def __init__(self, env=None):\n",
        "#         super(ProcessFrame84, self).__init__(env)\n",
        "#         self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "#     def observation(self, obs):\n",
        "#         return ProcessFrame84.process(obs)\n",
        "\n",
        "#     def process(frame):\n",
        "#         if frame.size == 240 * 256 * 3:\n",
        "#             img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "#         else:\n",
        "#             assert False, \"Unknown resolution.\"\n",
        "#         img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "#         resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "#         x_t = resized_screen[18:102, :]\n",
        "#         x_t = np.reshape(x_t, [84, 84, 1])\n",
        "#         return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "# class ImageToPyTorch(gym.ObservationWrapper):\n",
        "#     def __init__(self, env):\n",
        "#         super(ImageToPyTorch, self).__init__(env)\n",
        "#         old_shape = self.observation_space.shape\n",
        "#         self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "#                                                 dtype=np.float32)\n",
        "\n",
        "#     def observation(self, observation):\n",
        "#         return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "# class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "#     \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
        "#     def observation(self, obs):\n",
        "#         return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# class BufferWrapper(gym.ObservationWrapper):\n",
        "#     def __init__(self, env, n_steps, dtype=np.float32):\n",
        "#         super(BufferWrapper, self).__init__(env)\n",
        "#         self.dtype = dtype\n",
        "#         old_space = env.observation_space\n",
        "#         self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "#                                                 old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "#     def reset(self):\n",
        "#         self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "#         return self.observation(self.env.reset())\n",
        "\n",
        "#     def observation(self, observation):\n",
        "#         self.buffer[:-1] = self.buffer[1:]\n",
        "#         self.buffer[-1] = observation\n",
        "#         return self.buffer\n",
        "\n",
        "\n",
        "# def make_env(env, frame_stack):\n",
        "#     env = MaxAndSkipEnv(env)\n",
        "#     env = ProcessFrame84(env)\n",
        "#     env = ImageToPyTorch(env)\n",
        "#     env = BufferWrapper(env, 4)\n",
        "#     env = ScaledFloatFrame(env)\n",
        "#     return JoypadSpace(env, SIMPLE_MOVEMENT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7G2LZdyXqGG"
      },
      "outputs": [],
      "source": [
        "# Custom class to make end of life = end of episode\n",
        "# class SkipFrame(gym.Wrapper):\n",
        "\n",
        "#   def __init__(self, env, n):\n",
        "#     super(SkipFrame, self).__init__(env)\n",
        "#     self._n = n\n",
        "  \n",
        "#   def step(self, action):\n",
        "#     sum_rewards = 0\n",
        "    \n",
        "#     # Re-run every step in the n-frames and accumulate the info to return\n",
        "#     done = False\n",
        "#     for i in range(self._n):\n",
        "#       #print('before step: ', done)\n",
        "#       state, reward, done, info = self.env.step(action)\n",
        "#       #print('after step: ', done)\n",
        "#       sum_rewards += reward\n",
        "#       if done:\n",
        "#         break\n",
        "  \n",
        "    # return state, sum_rewards, done, info\n",
        "\n",
        "# from atari_wrappers: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip       = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "# from atari_wrappers: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped._life\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n",
        "            # so it's important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped._life\n",
        "        return obs\n",
        "\n",
        "class CustomReward(gym.Wrapper):\n",
        "  def __init__(self, env):\n",
        "    super(CustomReward, self).__init__(env)\n",
        "    self.score = 0\n",
        "    self.y_pos = 0\n",
        "    self.coins = 0\n",
        "\n",
        "  def step(self, action):\n",
        "    state, reward, done, info = self.env.step(action)\n",
        "    # original reward function: (new_x_pos - old_x_pos) + (new_time - old_time) + dealth_pealty(-15 if dead; 0 otherwise)\n",
        "    # clipped at (-15,15)\n",
        "    \n",
        "    # encourage Mario to jump to higher platforms (+5 reward if Mario stayes in y_pos >= 6 for consecutive frames)\n",
        "    if (info['y_pos'] >= 6 and self.y_pos >= 6):\n",
        "      reward += 5\n",
        "\n",
        "    # reward Mario for killing monsters (+reward = (change in coins between this frame and last frame/100))\n",
        "    reward += (info['coins'] - self.coins) / 100\n",
        "\n",
        "    # info: {'coins': (int), \n",
        "    #        'flag_get': (bool), \n",
        "    #        'life': (int), \n",
        "    #        'stage': (int), \n",
        "    #        'status': (str) {'small', 'tall', 'firecall'}, \n",
        "    #        'time': (int) time left, \n",
        "    #        'world': (int) {1,...,8}, \n",
        "    #        'x_pos': (int), \n",
        "    #        'y_pos': (int)}\n",
        "\n",
        "    # reward Mario for getting an upgrade\n",
        "    if info['status'] != 'small':\n",
        "      reward += 5\n",
        "\n",
        "    if info['flag_get']:\n",
        "      reward = 15\n",
        "    self.y_pos = info['y_pos']\n",
        "    self.coins = info['coins']\n",
        "    \n",
        "    return state, reward, done, info\n",
        "\n",
        "class ScaleReshapeEnv(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ScaleReshapeEnv, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, \n",
        "                                                high=255.0, \n",
        "                                                shape=(old_shape[1], old_shape[2], old_shape[0]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = np.array(observation).astype(np.float32) / 255.0\n",
        "        # print('before conversion ', observation.shape)\n",
        "        shape = self.observation_space.shape\n",
        "        observation = np.reshape(observation, (shape))\n",
        "        return observation\n",
        "\n",
        "\n",
        "def make_env(env, frame_size, frame_skip, frame_stack):\n",
        "  env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "  env = EpisodicLifeEnv(env)\n",
        "  env = CustomReward(env) # customise reward\n",
        "  # env = SkipFrame(env, frame_skip) \n",
        "  env = MaxAndSkipEnv(env, frame_skip) \n",
        "  env = GrayScaleObservation(env)\n",
        "  env = ResizeObservation(env, frame_size) # downsample to (84,84) like DeepMind\n",
        "  env = FrameStack(env, frame_stack)\n",
        "  env = ScaleReshapeEnv(env) # normalise and reshape (e.g. (4,84,84) --> (84,84,4))\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwzpafmhrp9S"
      },
      "outputs": [],
      "source": [
        "class UniformExperienceReplay(object):\n",
        "  def __init__(self, replay_size, mem_size):\n",
        "    self.sample_size = replay_size\n",
        "    self.mem_size = mem_size\n",
        "    self.buffer = [None] * self.mem_size\n",
        "    self.index = 0\n",
        "  \n",
        "  def push(self, s, a, r, d, s2):\n",
        "    self.buffer[self.index] = s, a, r, d, s2\n",
        "    self.index += 1\n",
        "\n",
        "    # reset if memory is full\n",
        "    if self.index == self.mem_size:\n",
        "      self.index = 0\n",
        "      self.buffer = [None] * self.mem_size\n",
        "  \n",
        "  def sample_exp(self):\n",
        "    s = [None] * self.sample_size\n",
        "    a = [None] * self.sample_size\n",
        "    r = [None] * self.sample_size\n",
        "    d = [None] * self.sample_size\n",
        "    s2 = [None] * self.sample_size\n",
        "\n",
        "    for batch, sample in enumerate(np.random.randint(0, self.index-1, self.sample_size)):\n",
        "      s_sample, a_sample, r_sample, d_sample, s2_sample = self.buffer[sample]\n",
        "      s[batch] = np.array(s_sample)\n",
        "      a[batch] = a_sample\n",
        "      r[batch] = r_sample\n",
        "      d[batch] = d_sample\n",
        "      s2[batch] = np.array(s2_sample)\n",
        "\n",
        "    return np.array(s), np.array(a, dtype=int), np.array(r, dtype=float), np.array(d, dtype=bool), np.array(s2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN89wNQhTR3z"
      },
      "outputs": [],
      "source": [
        "from os import X_OK\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Lambda, Multiply\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.backend import backend as K\n",
        "\n",
        "class DDQNMario:\n",
        "\n",
        "    def __init__(self, state_space, action_space, mem_size, frame_stack, replay_size, gamma, lr, max_epsilon, min_epsilon, epsilon_decay, sync_rate, pretrained, burnin, load_model_epi):\n",
        "      \n",
        "      self.pretrained = pretrained \n",
        "\n",
        "      # Learning variables\n",
        "      self.gamma = gamma\n",
        "      self.epsilon_decay = epsilon_decay\n",
        "      self.min_epsilon = min_epsilon\n",
        "      self.epsilon = max_epsilon\n",
        "      self.huber = keras.losses.Huber()\n",
        "      self.burnin = burnin\n",
        "\n",
        "      # Env variables\n",
        "      self.step = 0\n",
        "      self.state_space = state_space\n",
        "      self.num_actions = action_space\n",
        "      self.frame_stack = frame_stack\n",
        "\n",
        "      # Experience replay variables\n",
        "      self.mem_size = mem_size\n",
        "      self.sync_rate = sync_rate\n",
        "      self.buffer = 0\n",
        "      self.replay_size = replay_size\n",
        "      self.replayer = UniformExperienceReplay(replay_size, self.mem_size)\n",
        "      # self.action_mask = np.ones(self.num_actions, dtype=int)\n",
        "\n",
        "\n",
        "      # CNN network variables\n",
        "      self.lr = lr\n",
        "      self.optimiser = Adam(learning_rate=self.lr, clipnorm=1.0)\n",
        "      self.online_hist = []\n",
        "\n",
        "      # Init CNN networks\n",
        "      if self.pretrained:\n",
        "        # (Testing) use pre-trained weights for the models\n",
        "        self.online = load_model('online_' + load_model_epi)\n",
        "        self.target = load_model('target_' + load_model_epi)\n",
        "\n",
        "      else:\n",
        "        # (Training) inti models\n",
        "        self.online = self.create_model()\n",
        "        self.target = self.create_model()\n",
        "\n",
        "      # Logging\n",
        "      self.last_position = 0\n",
        "      self.epi_exploitation = 0\n",
        "\n",
        "    def create_model(self):\n",
        "      inputs = Input(shape=(self.state_space))\n",
        "      x = Conv2D(32, 3, strides=4, activation='relu', name='conv1')(inputs)\n",
        "      x = Conv2D(64, 3, strides=2,  activation='relu', name='conv2')(x)\n",
        "      x = Conv2D(64, 3, strides=1, activation='relu', name='conv3')(x)\n",
        "      x = Flatten()(x)\n",
        "      x = Dense(512, activation='relu')(x)\n",
        "      action = Dense(self.num_actions, activation='softmax', name='action')(x) \n",
        "\n",
        "      model = tf.keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "      model.compile(optimizer=self.optimiser, loss=self.huber)\n",
        "      #model.summary()\n",
        "      return model\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      self.step += 1\n",
        "\n",
        "      # Epsilon-greedy policy #\n",
        "      # Exploration \n",
        "      if random.random() < self.epsilon:  \n",
        "        return random.randrange(self.num_actions)\n",
        "      \n",
        "      # Exploitation\n",
        "      else:\n",
        "        state = state[np.newaxis, :, :, :] # (84,84,4) --> (1,84,84,4)\n",
        "        state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        actions = self.online.predict(state)\n",
        "        # print(np.argmax(actions))\n",
        "        self.epi_exploitation += 1\n",
        "\n",
        "        return np.argmax(actions[0])\n",
        "\n",
        "    def sync_models(self):\n",
        "      self.target.set_weights(self.online.get_weights())\n",
        "\n",
        "    def experience_replay(self, ep_num):\n",
        "      # reference: https://colab.research.google.com/github/ehennis/ReinforcementLearning/blob/master/06-DDQN.ipynb#scrollTo=4oVZuEP5vVTP\n",
        "      if self.step % self.sync_rate == 0:\n",
        "        self.sync_models()\n",
        "\n",
        "      if self.replayer.index < self.replayer.sample_size:\n",
        "        return \n",
        "\n",
        "      s = []\n",
        "      y = [] \n",
        "\n",
        "      # sample minibatch of state, action, reward, done and state2 \n",
        "      state_sample, action_sample, reward_sample, done_sample, state2_sample = self.replayer.sample_exp()\n",
        "\n",
        "      state_sample = tf.convert_to_tensor(state_sample)\n",
        "      state2_sample = tf.convert_to_tensor(state2_sample)\n",
        "\n",
        "      # predict in batch\n",
        "      state_pred_online = self.online.predict(state_sample) \n",
        "      state2_pred_online = self.online.predict(state2_sample)\n",
        "      state2_pred_target = self.target.predict(state2_sample)\n",
        "\n",
        "      for i in range(len(action_sample)):\n",
        "        s.append(state_sample[i])\n",
        "        \n",
        "        state2_action_pred_online = state2_pred_online[i]\n",
        "        state2_action_pred_target = state2_pred_target[i]\n",
        "      \n",
        "        if done_sample[i]:\n",
        "          target = reward_sample[i] # y_j = r_j + 0 if terminal\n",
        "        else:\n",
        "          target = reward_sample[i] + self.gamma * state2_action_pred_target[np.argmax(state2_action_pred_online)] \n",
        "        \n",
        "        # y_j\n",
        "        target_f = state_pred_online[i] # prediction result (array of size 7)\n",
        "        target_f[action_sample[i]] = target # \n",
        "        y.append(target_f)\n",
        "\n",
        "      # s = np.array(s)\n",
        "      # y = np.array(y)\n",
        "\n",
        "      s = tf.convert_to_tensor(s, dtype=tf.float32)\n",
        "      y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "      \n",
        "      # perform single gradient update based on y_j\n",
        "      self.online_hist = self.online.train_on_batch(s, y)\n",
        "\n",
        "      # Update epsilon \n",
        "      self.epsilon = max(self.min_epsilon, self.epsilon*self.epsilon_decay)\n",
        "\n",
        "    def log(self, state, action, reward, done, state2):\n",
        "      # print('shape of state logged: ', state.shape)\n",
        "      self.replayer.push(state, action, reward, done, state2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvyBJB3LTVph"
      },
      "outputs": [],
      "source": [
        "def show_state(env, ep=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"Episode: %d %s\" % (ep, info))\n",
        "    plt.axis('off')\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh12HV81TW4M"
      },
      "outputs": [],
      "source": [
        "frame_stack = 4\n",
        "def run(training_mode, pretrained, stage, model_epi=None):\n",
        "   \n",
        "\n",
        "\n",
        "    env = gym_super_mario_bros.make(stage) # world 1, stage 1, standard ROM\n",
        "    env = make_env(env, \n",
        "                   frame_size=84,\n",
        "                   frame_skip=4,\n",
        "                   frame_stack=frame_stack)  # Wraps the environment so that frames are grayscale \n",
        "    state_space = env.observation_space.shape\n",
        "    # print('observation space: ', env.observation_space.shape)\n",
        "    action_space = env.action_space.n\n",
        "\n",
        "    # Init\n",
        "    num_episodes = 300\n",
        "    burnin = int(num_episodes * 0.05)\n",
        "    env.reset()\n",
        "    total_rewards = []\n",
        "    exploitation_rate = []\n",
        "\n",
        "    agent = DDQNMario(state_space=state_space,\n",
        "                      action_space=action_space,\n",
        "                      frame_stack=frame_stack,\n",
        "                      mem_size=15000,\n",
        "                      replay_size=64,\n",
        "                      gamma=0.90,\n",
        "                      lr=.00025,\n",
        "                      max_epsilon=1.0,\n",
        "                      min_epsilon=0.1,\n",
        "                      epsilon_decay=0.9999,\n",
        "                      sync_rate = 10000, \n",
        "                      burnin = burnin,\n",
        "                      pretrained=pretrained, \n",
        "                      load_model_epi=model_epi)\n",
        "    \n",
        "    best_reward = 0\n",
        "    \n",
        "    for ep_num in tqdm(range(num_episodes)):\n",
        "        state = env.reset()\n",
        "        # print(state)\n",
        "        state = state\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        agent.epi_exploitation = 0\n",
        "\n",
        "        while True:\n",
        "\n",
        "            if not training_mode: \n",
        "                show_state(env, ep_num)\n",
        "            # print('choose action')\n",
        "            action = agent.choose_action(state)\n",
        "            # print('state shape fed into choose action',state.shape)\n",
        "            steps += 1\n",
        "            \n",
        "            state2, reward, done, info = env.step(int(action))\n",
        "            # print(state2)\n",
        "            # print('env shape', env.observation_space.shape)\n",
        "            # print('shape of state returned in step:', state2.shape)\n",
        "            total_reward += reward\n",
        "            state2 = state2\n",
        "            \n",
        "            if training_mode:\n",
        "                agent.log(state, action, reward, done, state2)\n",
        "                agent.experience_replay(ep_num)\n",
        "            \n",
        "            state = state2\n",
        "\n",
        "            if done:\n",
        "                exploitation_rate.append(agent.epi_exploitation / steps)\n",
        "                break\n",
        "        \n",
        "        total_rewards.append(total_reward)\n",
        "\n",
        "        print(\"Total reward after episode {} is {}. Exploitation rate is {}%.\".format(ep_num + 1, total_rewards[-1], exploitation_rate[ep_num]*100))\n",
        "        num_episodes += 1      \n",
        "\n",
        "        # save the best models so far (with rewards > 40000)\n",
        "        if total_reward > best_reward:\n",
        "          best_reward = total_reward\n",
        "          if total_reward > 40000:\n",
        "            agent.online.save(\"online_\"+ str(ep_num+1))\n",
        "            agent.target.save(\"target_\" + str(ep_num+1))\n",
        "\n",
        "    if training_mode:\n",
        "        # save training record \n",
        "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.last_position, f, protocol=4)\n",
        "        with open(\"buffer.pkl\", \"wb\") as f:\n",
        "            pickle.dump(agent.replayer.buffer, f, protocol=4)\n",
        "        \n",
        "        agent.online.save(\"online\")\n",
        "        agent.target.save(\"target\")\n",
        "\n",
        "    env.close()\n",
        "    \n",
        "    if num_episodes > burnin:\n",
        "        plt.title(\"Episodes trained vs. Average Rewards\")\n",
        "        plt.plot([0 for _ in range(burnin)] + \n",
        "            np.convolve(total_rewards, np.ones((burnin,))/burnin, mode=\"valid\").tolist())\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPNdhPcz1WEc",
        "outputId": "2a473aac-afe8-402a-d3a1-a1069e9a1ccc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/300 [01:04<5:22:51, 64.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 1 is 7848.02. Exploitation rate is 2.8089887640449436%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 2/300 [02:58<7:46:05, 93.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 2 is 11181.98. Exploitation rate is 5.2734375%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 3/300 [03:38<5:41:26, 68.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 3 is 4142.0. Exploitation rate is 7.954545454545454%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|▏         | 4/300 [04:22<4:52:09, 59.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 4 is 4744.0. Exploitation rate is 12.18274111675127%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 5/300 [04:33<3:25:11, 41.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 5 is 1169.0. Exploitation rate is 12.76595744680851%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 6/300 [07:23<6:59:14, 85.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 6 is 15938.0. Exploitation rate is 14.190093708165996%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 7/300 [07:59<5:38:22, 69.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 7 is 3788.0. Exploitation rate is 15.09433962264151%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 8/300 [12:24<10:40:33, 131.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 8 is 23859.010000000002. Exploitation rate is 25.505716798592786%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 9/300 [12:52<8:00:55, 99.16s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 9 is 2956.99. Exploitation rate is 35.59322033898305%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 10/300 [14:14<7:33:58, 93.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 10 is 8000.0. Exploitation rate is 28.857142857142858%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 11/300 [17:05<9:25:04, 117.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 11 is 15447.02. Exploitation rate is 33.56258596973865%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 12/300 [19:09<9:33:55, 119.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 12 is 11007.98. Exploitation rate is 36.03082851637765%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 13/300 [19:46<7:31:18, 94.35s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 13 is 3636.01. Exploitation rate is 43.13725490196079%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 14/300 [20:48<6:43:14, 84.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 14 is 5938.01. Exploitation rate is 37.786259541984734%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 15/300 [20:56<4:52:05, 61.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 15 is 922.98. Exploitation rate is 38.23529411764706%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 16/300 [22:38<5:49:39, 73.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 16 is 9638.02. Exploitation rate is 44.651162790697676%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 17/300 [22:50<4:20:26, 55.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 17 is 1201.99. Exploitation rate is 46.93877551020408%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 18/300 [24:44<5:42:09, 72.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 18 is 10538.0. Exploitation rate is 46.62447257383967%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▋         | 19/300 [26:07<5:55:30, 75.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 19 is 7773.0. Exploitation rate is 42.57142857142857%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 20/300 [26:41<4:55:06, 63.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 20 is 2915.99. Exploitation rate is 49.64028776978417%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 21/300 [27:20<4:19:57, 55.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 21 is 3749.0. Exploitation rate is 56.60377358490566%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 22/300 [31:11<8:22:33, 108.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 22 is 26757.02. Exploitation rate is 52.32558139534884%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 23/300 [33:53<9:35:10, 124.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 23 is 14189.99. Exploitation rate is 55.80693815987934%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 24/300 [36:51<10:46:56, 140.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 24 is 17987.989999999998. Exploitation rate is 58.97079276773296%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 25/300 [38:54<10:20:50, 135.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 25 is 10694.0. Exploitation rate is 59.63855421686747%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▊         | 26/300 [39:16<7:42:17, 101.23s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 26 is 1920.0. Exploitation rate is 54.02298850574713%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 27/300 [39:49<6:07:27, 80.76s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 27 is 2779.0. Exploitation rate is 61.06870229007634%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 28/300 [40:04<4:36:56, 61.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 28 is 1442.0. Exploitation rate is 59.01639344262295%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|▉         | 29/300 [40:25<3:41:53, 49.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 29 is 1876.0. Exploitation rate is 65.88235294117646%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 30/300 [41:24<3:54:33, 52.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 30 is 5267.0. Exploitation rate is 65.2542372881356%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 31/300 [45:08<7:45:14, 103.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 31 is 22966.010000000002. Exploitation rate is 65.73660714285714%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 32/300 [49:22<11:03:41, 148.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 32 is 36325.01. Exploitation rate is 70.7171314741036%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 33/300 [49:49<8:19:41, 112.29s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 33 is 2435.9900000000002. Exploitation rate is 60.71428571428571%.\n",
            "Total reward after episode 34 is 68814.0. Exploitation rate is 72.62813522355506%.\n",
            "INFO:tensorflow:Assets written to: online_34/assets\n",
            "INFO:tensorflow:Assets written to: target_34/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 35/300 [58:24<12:20:27, 167.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 35 is 3901.99. Exploitation rate is 76.84210526315789%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 36/300 [1:03:15<15:00:15, 204.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 36 is 36891.020000000004. Exploitation rate is 78.54113655640373%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 37/300 [1:09:24<18:32:52, 253.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 37 is 29223.0. Exploitation rate is 79.83310152990263%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 38/300 [1:10:03<13:47:14, 189.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 38 is 3136.98. Exploitation rate is 82.35294117647058%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 39/300 [1:17:25<19:13:26, 265.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 39 is 46759.01. Exploitation rate is 82.98245614035088%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 40/300 [1:26:07<24:42:30, 342.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 40 is 40402.0. Exploitation rate is 87.08229426433915%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▎        | 41/300 [1:27:32<19:03:57, 265.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 41 is 6530.99. Exploitation rate is 87.07692307692308%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 42/300 [1:28:46<14:53:52, 207.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 42 is 5773.0. Exploitation rate is 88.7719298245614%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 43/300 [1:37:36<21:43:11, 304.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 43 is 55979.020000000004. Exploitation rate is 89.35643564356435%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▍        | 44/300 [1:39:03<17:00:11, 239.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 44 is 6720.98. Exploitation rate is 89.52095808383234%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 45/300 [1:40:11<13:18:35, 187.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 45 is 5211.0. Exploitation rate is 89.92248062015504%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 46/300 [1:49:03<20:31:55, 291.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 46 is 62957.020000000004. Exploitation rate is 90.0990099009901%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 47/300 [1:49:33<14:57:54, 212.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 47 is 2428.98. Exploitation rate is 88.13559322033898%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 48/300 [1:58:22<21:31:36, 307.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 48 is 40194.020000000004. Exploitation rate is 89.47630922693267%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▋        | 49/300 [2:07:06<25:58:49, 372.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 49 is 67498.0. Exploitation rate is 90.3960396039604%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 50/300 [2:15:57<29:10:20, 420.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 50 is 40361.99. Exploitation rate is 90.67331670822942%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 51/300 [2:24:49<31:22:45, 453.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 51 is 65584.01. Exploitation rate is 89.35643564356435%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 52/300 [2:27:08<24:45:05, 359.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 52 is 10501.98. Exploitation rate is 90.30418250950571%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 53/300 [2:28:52<19:23:38, 282.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 53 is 7930.0. Exploitation rate is 89.59390862944161%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 54/300 [2:37:47<24:29:09, 358.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 54 is 48971.020000000004. Exploitation rate is 90.2970297029703%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 55/300 [2:39:23<19:02:27, 279.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 55 is 7422.98. Exploitation rate is 85.63685636856368%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▊        | 56/300 [2:41:17<15:34:51, 229.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 56 is 8623.0. Exploitation rate is 92.5233644859813%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▉        | 57/300 [2:42:49<12:44:15, 188.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 57 is 7111.0. Exploitation rate is 86.19718309859155%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▉        | 58/300 [2:51:40<19:34:30, 291.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 58 is 57807.009999999995. Exploitation rate is 88.36633663366337%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|█▉        | 59/300 [2:52:23<14:30:57, 216.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 59 is 3367.99. Exploitation rate is 90.9090909090909%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 60/300 [2:54:45<12:57:38, 194.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 60 is 10848.0. Exploitation rate is 89.64879852125694%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 61/300 [3:03:33<19:32:28, 294.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 61 is 40327.020000000004. Exploitation rate is 89.62593516209476%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██        | 62/300 [3:12:12<23:55:17, 361.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 62 is 57763.0. Exploitation rate is 89.95049504950495%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██        | 63/300 [3:12:44<17:17:48, 262.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 63 is 2492.98. Exploitation rate is 87.5%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██▏       | 64/300 [3:14:00<13:33:19, 206.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 64 is 5785.0. Exploitation rate is 93.05555555555556%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 65/300 [3:15:58<11:46:22, 180.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 65 is 9025.0. Exploitation rate is 89.15929203539822%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 66/300 [3:17:18<9:45:57, 150.24s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 66 is 6113.0. Exploitation rate is 91.11842105263158%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 67/300 [3:20:02<9:58:27, 154.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 67 is 12393.0. Exploitation rate is 90.64516129032259%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 68/300 [3:28:51<17:11:47, 266.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 68 is 60198.02. Exploitation rate is 90.0%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 69/300 [3:37:42<22:12:03, 345.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 69 is 62879.99. Exploitation rate is 90.44554455445545%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 70/300 [3:46:33<25:39:01, 401.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 70 is 67770.98999999999. Exploitation rate is 90.14851485148515%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▎       | 71/300 [3:48:10<19:43:33, 310.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 71 is 7513.0. Exploitation rate is 90.81081081081082%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 72/300 [3:56:54<23:42:25, 374.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 72 is 40202.0. Exploitation rate is 89.62593516209476%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 73/300 [3:59:00<18:54:30, 299.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 73 is 9636.0. Exploitation rate is 90.87136929460581%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▍       | 74/300 [3:59:59<14:16:51, 227.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 74 is 4468.0. Exploitation rate is 90.54054054054053%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 75/300 [4:08:43<19:47:10, 316.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 75 is 40399.01. Exploitation rate is 89.22693266832917%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 76/300 [4:10:01<15:13:51, 244.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 76 is 5921.99. Exploitation rate is 89.45578231292517%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 77/300 [4:11:24<12:09:24, 196.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 77 is 6335.0. Exploitation rate is 91.40127388535032%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 78/300 [4:13:29<10:46:54, 174.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 78 is 9472.0. Exploitation rate is 89.87341772151899%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▋       | 79/300 [4:14:20<8:28:10, 137.96s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 79 is 4076.0. Exploitation rate is 85.0%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 80/300 [4:15:47<7:29:47, 122.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 80 is 6593.0. Exploitation rate is 88.18181818181819%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 81/300 [4:16:52<6:24:36, 105.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 81 is 5976.0. Exploitation rate is 89.26174496644296%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 82/300 [4:25:01<13:20:32, 220.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 82 is 62494.009999999995. Exploitation rate is 89.50187466523835%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 83/300 [4:33:50<18:51:43, 312.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 83 is 53070.99. Exploitation rate is 90.24752475247524%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 84/300 [4:38:21<18:01:35, 300.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 84 is 20541.0. Exploitation rate is 91.19922630560929%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 85/300 [4:41:01<15:25:43, 258.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 85 is 12237.0. Exploitation rate is 89.88580750407831%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 29%|██▊       | 86/300 [4:42:43<12:34:10, 211.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 86 is 7830.0. Exploitation rate is 90.25641025641026%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 87/300 [4:44:47<10:57:28, 185.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 87 is 9491.0. Exploitation rate is 89.26315789473685%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 88/300 [4:53:34<16:56:42, 287.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 88 is 61677.009999999995. Exploitation rate is 89.95049504950495%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|██▉       | 89/300 [5:02:21<21:03:50, 359.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 89 is 58704.0. Exploitation rate is 89.4059405940594%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 90/300 [5:02:52<15:13:29, 261.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 90 is 2548.99. Exploitation rate is 87.60330578512396%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|███       | 91/300 [5:04:10<11:57:35, 206.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 91 is 5978.0. Exploitation rate is 90.93959731543623%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 92/300 [5:06:55<11:11:17, 193.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 92 is 12658.0. Exploitation rate is 89.0995260663507%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 93/300 [5:15:39<16:50:26, 292.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 93 is 40229.009999999995. Exploitation rate is 90.42394014962593%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███▏      | 94/300 [5:16:31<12:36:55, 220.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 94 is 3977.99. Exploitation rate is 91.7948717948718%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 95/300 [5:17:12<9:29:44, 166.76s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 95 is 3202.0. Exploitation rate is 90.50632911392405%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 96/300 [5:18:37<8:03:33, 142.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 96 is 6436.0. Exploitation rate is 93.76947040498442%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 97/300 [5:27:12<14:19:24, 254.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 97 is 40121.020000000004. Exploitation rate is 90.47381546134663%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 98/300 [5:35:20<18:11:17, 324.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 98 is 37514.0. Exploitation rate is 89.8815931108719%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 99/300 [5:36:24<13:44:37, 246.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 99 is 5006.98. Exploitation rate is 87.4493927125506%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 100/300 [5:45:20<18:30:28, 333.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 100 is 61392.020000000004. Exploitation rate is 90.14851485148515%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 34%|███▎      | 101/300 [5:54:12<21:42:51, 392.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 101 is 61028.99. Exploitation rate is 90.1980198019802%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 102/300 [5:56:05<16:59:18, 308.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 102 is 8705.99. Exploitation rate is 87.52886836027713%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 103/300 [5:56:55<12:39:08, 231.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 103 is 3915.0. Exploitation rate is 87.5%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▍      | 104/300 [5:58:02<9:54:05, 181.86s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 104 is 5123.0. Exploitation rate is 87.890625%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 105/300 [6:06:51<15:29:24, 285.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 105 is 44954.02. Exploitation rate is 90.0%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 106/300 [6:15:35<19:16:03, 357.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 106 is 40202.99. Exploitation rate is 89.92518703241895%.\n",
            "Total reward after episode 107 is 69095.01. Exploitation rate is 88.91089108910892%.\n",
            "INFO:tensorflow:Assets written to: online_107/assets\n",
            "INFO:tensorflow:Assets written to: target_107/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 108/300 [6:25:15<16:04:16, 301.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 108 is 3879.98. Exploitation rate is 89.47368421052632%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 36%|███▋      | 109/300 [6:25:55<11:50:06, 223.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 109 is 3202.0. Exploitation rate is 90.25974025974025%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 110/300 [6:27:08<9:24:07, 178.15s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 110 is 6006.0. Exploitation rate is 87.45762711864407%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 111/300 [6:29:02<8:19:41, 158.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 111 is 9294.0. Exploitation rate is 91.8103448275862%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 112/300 [6:29:39<6:22:42, 122.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward after episode 112 is 2867.0. Exploitation rate is 93.5251798561151%.\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "run(training_mode=True, pretrained=False, stage='SuperMarioBros-1-1-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CydQkW-_bOiu"
      },
      "outputs": [],
      "source": [
        "# Test model \n",
        "run(training_mode=False, pretrained=True, stage='SuperMarioBros-1-1-v0', model_epi='32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHimu5_XQj5a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "rl-super-mario-bros.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}